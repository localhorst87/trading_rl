{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython\n",
    "import sys\n",
    "sys.path.append('/home/localhorst87/Documents/coding/rl/finance/trading_rl/lib/') # edit with your lib path\n",
    "\n",
    "from DataProvider import *\n",
    "from Environment import *\n",
    "from Learning import *\n",
    "import Rewarder\n",
    "import ActionSpace\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '/home/localhorst87/Documents/coding/rl/finance/quant_hour_data.p' # edit with your data path\n",
    "windowLength = 50\n",
    "\n",
    "actionSpace = ActionSpace.SeparatedNets()\n",
    "rewarder = Rewarder.WeightedMaxMinOpeningRewarder()\n",
    "env = Environment(dataPath, windowLength, actionSpace, rewarder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent = OpeningAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "meanRewards = []\n",
    "trainingLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSizeTraining = 50\n",
    "agent.batchSize = 256 # don't use small batches as we treat with a lot of noise in by optimizing a gaussian distribution!\n",
    "agent.learningRate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.minRewardTarget = 0.00\n",
    "agent.probabilityThreshold = 0.50\n",
    "agent.randomActionProbability = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100000): \n",
    "    \n",
    "    reward, action, loss = agent.runEpisode()\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    meanRewards.append(np.mean(rewards[-500:]))\n",
    "    if loss is not None: trainingLoss.append(loss)\n",
    "\n",
    "    if agent.episode > 10000 and agent.randomActionProbability >= 0.05:\n",
    "        agent.randomActionProbability -= 0.95/40000\n",
    "\n",
    "    if agent.episode > 10000 and agent.probabilityThreshold < 0.67:\n",
    "        agent.probabilityThreshold += (0.67-0.50)/50000\n",
    "\n",
    "    if agent.episode > 15000 and agent.minRewardTarget < 0.10:\n",
    "        agent.minRewardTarget += 0.10/50000\n",
    "    \n",
    "    print(str(agent.episode) + \") minReward:\" + str(round(agent.minRewardTarget,2)) + \\\n",
    "          \" probThresh:\" + str(round(agent.probabilityThreshold,2)) + \\\n",
    "          \" randomProb:\" + str(round(agent.randomActionProbability,2)) + \" --> \" + action)\n",
    "    \n",
    "    if agent.episode % 100 == 0:\n",
    "        \n",
    "        IPython.display.clear_output()\n",
    "    \n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(meanRewards[-5000:])\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(trainingLoss)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
