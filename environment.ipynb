{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/logo-small.png)\n",
    "## Welcome to The QuantConnect Research Page\n",
    "#### Refer to this page for documentation https://www.quantconnect.com/docs#Introduction-to-Jupyter\n",
    "#### Contribute to this template file https://github.com/QuantConnect/Lean/blob/master/Jupyter/BasicQuantBookTemplate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Imports\n",
    "from clr import AddReference\n",
    "AddReference(\"System\")\n",
    "AddReference(\"QuantConnect.Common\")\n",
    "AddReference(\"QuantConnect.Jupyter\")\n",
    "AddReference(\"QuantConnect.Indicators\")\n",
    "from System import *\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Data.Custom import *\n",
    "from QuantConnect.Data.Market import TradeBar, QuoteBar\n",
    "from QuantConnect.Data.Consolidators import QuoteBarConsolidator\n",
    "from QuantConnect.Jupyter import *\n",
    "from QuantConnect.Indicators import *\n",
    "from QuantConnect.Indicators.CandlestickPatterns import *\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, barsPerSequence, resolution = Resolution.Hour):\n",
    "        self.nSteps = 0\n",
    "        self.currentEpisode = 0\n",
    "        self.randomProbabilityStart = 0.40\n",
    "        self.randomProbabilityMin = 0.00\n",
    "        self.randomProbabilityEpisodesToMin = 400\n",
    "        self.randomProbability = self.randomProbabilityStart\n",
    "        self.minHoldTimeBarsStart = 20\n",
    "        self.minHoldTimeBarsFinal = 1\n",
    "        self.minHoldTimeBarsEpisodesToFinal = 200\n",
    "        self.minHoldTimeBars = self.minHoldTimeBarsStart\n",
    "        self.discountFactor = 0.95\n",
    "        self.barsPerSequence = 24\n",
    "        self.environment = Environment(barsPerSequence, resolution)\n",
    "        self.batchSize = 48\n",
    "        self.numInputSignals = 4  \n",
    "        self.stepSizeTraining = 10\n",
    "        self.stepsUntilFirstTraining = self.batchSize + 50\n",
    "        self.stepSizeCopyingWeights = 50\n",
    "        self.stepsUntilFirstCopy = self.stepsUntilFirstTraining + 50\n",
    "        self.numActions = self.environment.getNumberOfActions()\n",
    "        self.replayMemory = ReplayMemory(2000)\n",
    "        self.networkHandler = NetworkHandler(self.numInputSignals, self.barsPerSequence, self.numActions)\n",
    "    \n",
    "    def run(self, numEpisodes, session):\n",
    "        \n",
    "        self.networkHandler.session = session\n",
    "        if self.currentEpisode == 0: tf.global_variables_initializer().run(session = self.networkHandler.session)\n",
    "\n",
    "        for iEpisode in range(self.currentEpisode, self.currentEpisode + numEpisodes):\n",
    "\n",
    "            holdTimeBars = 0\n",
    "            self.currentEpisode += 1\n",
    "\n",
    "            observation, possibleActions, isDone = self.environment.reset()\n",
    "            nextAction = self.networkHandler.chooseAction(observation, possibleActions, self.randomProbability)\n",
    "            self.nSteps += 1\n",
    "\n",
    "            while not isDone:\n",
    "\n",
    "                if self.environment.currentSituation > 0: holdTimeBars += 1\n",
    "\n",
    "                observation, possibleActions, isDone = self.environment.step(nextAction)\n",
    "                if holdTimeBars < self.minHoldTimeBars:\n",
    "                    if self.environment.currentSituation == 1: possibleActions = [3]\n",
    "                    if self.environment.currentSituation == 2: possibleActions = [4]\n",
    "\n",
    "                nextAction = self.networkHandler.chooseAction(observation, possibleActions, self.randomProbability)\n",
    "                self.nSteps += 1\n",
    "\n",
    "                if self.isTrainingRequested(): self.networkHandler.trainNetwork(self.replayMemory, self.batchSize, self.discountFactor)\n",
    "                if self.isCopyRequested(): self.networkHandler.copyWeightsToQNet()\n",
    "\n",
    "            allSarsSamples = self.environment.getSarsSamples()\n",
    "            for sarsSample in allSarsSamples:\n",
    "                self.replayMemory.add(sarsSample)\n",
    "\n",
    "            self.decayRandomProb(iEpisode)\n",
    "            self.decayHoldTime(iEpisode)\n",
    "\n",
    "            print(\"episode \" + str(iEpisode+1) + \" finished. ROI: \" \\\n",
    "                  + str( round(100*self.environment.roiHistory[-1], 2)) )\n",
    "    \n",
    "    def isTrainingRequested(self):\n",
    "        if self.nSteps > self.stepsUntilFirstTraining and self.nSteps % self.stepSizeTraining == 0 and not self.replayMemory.isEmpty():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def isCopyRequested(self):\n",
    "        if self.nSteps > self.stepsUntilFirstCopy and self.nSteps % self.stepSizeCopyingWeights == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def decayRandomProb(self, nEpisode):\n",
    "        decayGradient = (self.randomProbabilityStart - self.randomProbabilityMin) / self.randomProbabilityEpisodesToMin\n",
    "        self.randomProbability -= decayGradient\n",
    "        self.randomProbability = max(self.randomProbability, self.randomProbabilityMin)\n",
    "    \n",
    "    def decayHoldTime(self, nEpisode):\n",
    "        holdTimeGradient = (self.minHoldTimeBarsStart - self.minHoldTimeBarsFinal) / self.minHoldTimeBarsEpisodesToFinal\n",
    "        self.minHoldTimeBars -= holdTimeGradient\n",
    "        self.minHoldTimeBars = max(self.minHoldTimeBars, self.minHoldTimeBarsFinal)\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "        self.experienceMemory = collections.deque(maxlen = length)\n",
    "        \n",
    "    def sampleMemories(self, batchSize):\n",
    "        \n",
    "        lengthMemory = len(self.experienceMemory)\n",
    "        if lengthMemory < batchSize: raise ValueError(\"not enough samples to create a batch of the desired size\")\n",
    "        permutedMemory = np.random.permutation(lengthMemory)[0 : batchSize]\n",
    "        sampledMemory = np.array(self.experienceMemory)[permutedMemory]\n",
    "    \n",
    "        observationBatch = sampledMemory[:, 0]\n",
    "        actionBatch = sampledMemory[:, 1]\n",
    "        rewardBatch = sampledMemory[:, 2]\n",
    "        observationNextBatch = sampledMemory[:, 3]\n",
    "        doneBatch = sampledMemory[:, 4] \n",
    "        \n",
    "        return observationBatch, actionBatch, rewardBatch, observationNextBatch, doneBatch\n",
    "        \n",
    "    def add(self, sample):\n",
    "        # sample is expected to be a [observation, action, reward, observationNext, done] list \n",
    "        \n",
    "        self.experienceMemory.append(sample)\n",
    "    \n",
    "    def isEmpty(self):\n",
    "        return len(self.experienceMemory) == 0\n",
    "    \n",
    "class NetworkHandler:\n",
    "    \n",
    "    def __init__(self, numInputSignals, barsPerSequence, numActions):\n",
    "        self.learningRate = 0.001\n",
    "        self.numInputSignals = numInputSignals\n",
    "        self.barsPerSequence = barsPerSequence\n",
    "        self.numActions = numActions\n",
    "        self.session = None\n",
    "        self.inputs = None\n",
    "        self.qNet = None\n",
    "        self.targetNet = None\n",
    "        self.initNetworks()\n",
    "        self.defineOptimization()   \n",
    "                \n",
    "    def initNetworks(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, self.numInputSignals, self.barsPerSequence])\n",
    "        self.qNet = DeepQNet(self.inputs, self.numActions, 'QNet')\n",
    "        self.qNet.buildNetwork()\n",
    "        self.targetNet = DeepQNet(self.inputs, self.numActions, 'TargetNet')\n",
    "        self.targetNet.buildNetwork()\n",
    "        \n",
    "    def chooseAction(self, observation, possibleActions, randomChance = 0):\n",
    "        # chooses the best action according to epsilon greedy strategy\n",
    "        \n",
    "        randomChance = min( 1, max(0, randomChance) ) \n",
    "        if randomChance > random.uniform(0, 1):\n",
    "            randomActionOfPossibleActions = random.randint(0, len(possibleActions) -1)\n",
    "            randomAction = possibleActions[randomActionOfPossibleActions]\n",
    "            return randomAction\n",
    "        else:\n",
    "            qValuesAction = self.qNet.output.eval(session = self.session, feed_dict = { self.inputs: [observation] } )\n",
    "            bestActionOfPossibleActions = np.argmax(np.take(qValuesAction, possibleActions))\n",
    "            bestAction = possibleActions[bestActionOfPossibleActions]\n",
    "            return bestAction\n",
    "            \n",
    "    def defineOptimization(self):\n",
    "        \n",
    "        self.actionPerformed = tf.placeholder(tf.int32, shape = (None,))\n",
    "        qValuePrediction = tf.reduce_sum( self.targetNet.output * tf.one_hot(self.actionPerformed, self.numActions), axis = -1, keepdims = True )\n",
    "        self.qValueTarget = tf.placeholder( tf.float32, shape = (None, 1) )\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "        self.loss = tf.reduce_mean( tf.square(self.qValueTarget - qValuePrediction) )\n",
    "        self.trainingOp = optimizer.minimize(self.loss)\n",
    "    \n",
    "    def trainNetwork(self, replayMemory, batchSize, gamma):\n",
    "        \n",
    "        observationBatch, actionBatch, rewardBatch, observationNextBatch, doneBatch = replayMemory.sampleMemories(batchSize)\n",
    "        \n",
    "        lstmInput = observationBatch.tolist()\n",
    "        lstmInputNext = observationNextBatch.tolist()\n",
    "        actionBatch = actionBatch.tolist()\n",
    "        \n",
    "        qValuesNextActionQNet = self.qNet.output\n",
    "        qValuesNextActionTargetNet = self.targetNet.output\n",
    "        bestNextAction = tf.argmax(qValuesNextActionQNet, axis = -1) # double DQN. Let main network choose the action ...\n",
    "        qValueBestNextAction = tf.reduce_sum( qValuesNextActionTargetNet * tf.one_hot(bestNextAction, self.numActions), \\\n",
    "                               axis = -1, keepdims = False) # ... and use the Q-value of the target network  \n",
    "        qValueTargetBatch = rewardBatch + (gamma * qValueBestNextAction) * (1 - doneBatch)\n",
    "        qValueTargetBatch = qValueTargetBatch.eval(session = self.session, feed_dict = { self.inputs: lstmInputNext } )\n",
    "        \n",
    "        trainLoss, _ = self.session.run( [self.loss, self.trainingOp], feed_dict = { self.inputs: lstmInput, \\\n",
    "                self.qValueTarget: np.expand_dims(qValueTargetBatch, axis = -1), self.actionPerformed: actionBatch} )\n",
    "    \n",
    "    def copyWeightsToQNet(self):\n",
    "        # copies the weights of the target network to the main q network\n",
    "        \n",
    "        copyOperations = []\n",
    "        \n",
    "        for variableName, networkVariable in self.qNet.networkVariables.items():\n",
    "            copyTensor = tf.assign( networkVariable, self.targetNet.networkVariables[variableName] )\n",
    "            copyOperations.append(copyTensor)\n",
    "        \n",
    "        tf.group(*copyOperations).run(session = self.session)\n",
    "\n",
    "class DeepQNet:\n",
    "    \n",
    "    def __init__(self, networkInput, numActions, networkName):\n",
    "        self.networkName = networkName\n",
    "        self.lstmUnits = 16\n",
    "        self.neuronsFcn1 = 64\n",
    "        self.numOutputs = numActions\n",
    "        self.networkInput = networkInput\n",
    "        self.output = None\n",
    "        self.networkVariables = None\n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        # 4x16 LSTM --> 16x64 FCN --> ReLu --> 64x7 FCN\n",
    "        \n",
    "        with tf.variable_scope(self.networkName) as scope:\n",
    "\n",
    "            lstmCell = tf.contrib.rnn.LSTMCell(self.lstmUnits)\n",
    "            wrappedLstmCell = tf.contrib.rnn.DropoutWrapper(cell = lstmCell, output_keep_prob = 0.8)\n",
    "            outputs, _ = tf.nn.dynamic_rnn(wrappedLstmCell, self.networkInput, dtype = tf.float32)  # shape: [batchSize, sequenceLength, lstmUnits]\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2]) # shape: [sequenceLength, batchSize, lstmUnits]\n",
    "            sequenceLength = int( outputs.get_shape()[0] )\n",
    "            lastSequenceOutputs = tf.gather( outputs, sequenceLength - 1) # returns [batchSize, lstmUnits] of the last sequence sample\n",
    "            \n",
    "            weightsInitFcn1 = tf.truncated_normal_initializer( stddev = math.sqrt( 2 / (self.lstmUnits + self.neuronsFcn1) ) )\n",
    "            biasInitFcn1 = tf.constant_initializer(0.05)\n",
    "            fcn1 = tf.contrib.layers.fully_connected( lastSequenceOutputs, num_outputs = self.neuronsFcn1, \\\n",
    "                                                     weights_initializer = weightsInitFcn1, biases_initializer = biasInitFcn1)\n",
    "            \n",
    "            weightsInitFcn2 = tf.truncated_normal_initializer( stddev = math.sqrt( 2 / (self.neuronsFcn1 + self.numOutputs) ) )\n",
    "            biasInitFcn2 = tf.constant_initializer(0.05)\n",
    "            self.output = tf.contrib.layers.fully_connected( fcn1, num_outputs = self.numOutputs, activation_fn = None, \\\n",
    "                                                            weights_initializer = weightsInitFcn2, biases_initializer = biasInitFcn2, scope = None )\n",
    "            \n",
    "            self.networkVariables = { trainable_var.name[ len(scope.name): ] : trainable_var for trainable_var in tf.trainable_variables(scope=scope.name) }\n",
    "\n",
    "class Environment:\n",
    "    ''' Provides the observations, performs the actions and yields the rewards.\n",
    "    Each time a new episode is started via the reset method, a random forex is selected at a random time,\n",
    "    given the time boundary conditions and selected portfolio of forex. '''\n",
    "    \n",
    "    def __init__(self, barsPerSequence, resolution):\n",
    "        self.actionSpace = {0: self.observeMarket, 1: self.orderLong, 2: self.orderShort, \\\n",
    "                            3: self.holdLong, 4: self.holdShort, 5: self.closeLong, 6: self.closeShort}\n",
    "        self.sampler = Sampler(resolution)\n",
    "        self.barsPerSequence = barsPerSequence\n",
    "        self.currentSample = None\n",
    "        self.possibleNextActions = [0, 1, 2]\n",
    "        self.sampleObservationHistory = []\n",
    "        self.sampleActionHistory = []\n",
    "        self.sampleRewardHistory = []\n",
    "        self.sampleDoneHistory = []\n",
    "        self.roiHistory = []\n",
    "        self.currentSituation = 0 # 0: no open position, 1: open long position, 2: open short position\n",
    "   \n",
    "    def reset(self):\n",
    "        # use this method to initialize a new episode\n",
    "        \n",
    "        self.possibleNextActions = [0, 1, 2]\n",
    "        self.sampleObservationHistory = []\n",
    "        self.sampleActionHistory = []\n",
    "        self.sampleRewardHistory = []\n",
    "        self.sampleDoneHistory = []\n",
    "        self.currentSituation = 0\n",
    "        self.currentSample = self.sampler.getSample(self.barsPerSequence)\n",
    "        self.currentSample.__iter__() # init the iteration\n",
    "        self.sampleObservationHistory.append( self.currentSample.__next__() )\n",
    "        \n",
    "        observation = self.getLastObservation()\n",
    "        isDone = False\n",
    "        \n",
    "        return observation, self.possibleNextActions, isDone\n",
    "    \n",
    "    def step(self, action):\n",
    "        # each step returns a new observation, reward and information if the episode is done\n",
    "        \n",
    "        if not self.isSampleDone():\n",
    "            try:\n",
    "                self.actionSpace[action]() # performs the action\n",
    "                self.sampleObservationHistory.append( self.currentSample.__next__() )\n",
    "                isDone = self.sampleDoneHistory[-1]\n",
    "            except StopIteration:\n",
    "                isDone = True\n",
    "        else:\n",
    "            isDone = True\n",
    "        \n",
    "        observation = self.getLastObservation()\n",
    "                \n",
    "        return observation, self.possibleNextActions, isDone\n",
    "    \n",
    "    def getNumberOfActions(self):\n",
    "        return len(self.actionSpace)\n",
    "    \n",
    "    def getSarsSamples(self):\n",
    "        # returns all the (s,a,r,s')-samples from the last episode\n",
    "        \n",
    "        nTransitions = len(self.sampleActionHistory)\n",
    "        sarsSamples = []\n",
    "        for i in range(0, nTransitions):\n",
    "            observation = self.sampleObservationHistory[i]\n",
    "            action = self.sampleActionHistory[i]\n",
    "            reward = self.sampleRewardHistory[i]\n",
    "            observationNext = self.sampleObservationHistory[i+1]\n",
    "            done = self.sampleDoneHistory[i]\n",
    "            sarsSamples.append([observation, action, reward, observationNext, done])\n",
    "        \n",
    "        return sarsSamples\n",
    "    \n",
    "    def getLastObservation(self):\n",
    "        return self.sampleObservationHistory[-1]\n",
    "    \n",
    "    def isSampleDone(self):\n",
    "        if len(self.sampleActionHistory) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return self.sampleActionHistory[-1] == 5 or self.sampleActionHistory[-1] == 6\n",
    "    \n",
    "    def observeMarket(self):\n",
    "        self.sampleActionHistory.append(0)\n",
    "        self.sampleDoneHistory.append(False)\n",
    "        self.possibleNextActions = [0, 1, 2]\n",
    "        self.currentSituation = 0\n",
    "    \n",
    "    def orderLong(self):\n",
    "        self.sampleActionHistory.append(1)\n",
    "        self.sampleDoneHistory.append(False)\n",
    "        self.possibleNextActions = [3, 5]\n",
    "        self.currentSituation = 1\n",
    "    \n",
    "    def orderShort(self):\n",
    "        self.sampleActionHistory.append(2)\n",
    "        self.sampleDoneHistory.append(False)\n",
    "        self.possibleNextActions = [4, 6]\n",
    "        self.currentSituation = 2\n",
    "    \n",
    "    def holdLong(self):\n",
    "        self.sampleActionHistory.append(3)\n",
    "        self.sampleDoneHistory.append(False)\n",
    "        self.possibleNextActions = [3, 5]\n",
    "        self.currentSituation = 1\n",
    "        \n",
    "    def holdShort(self):\n",
    "        self.sampleActionHistory.append(4)\n",
    "        self.sampleDoneHistory.append(False)\n",
    "        self.possibleNextActions = [4, 6]\n",
    "        self.currentSituation = 2\n",
    "        \n",
    "    def closeLong(self):\n",
    "        self.sampleActionHistory.append(5)\n",
    "        self.closePosition()\n",
    "        self.possibleNextActions = [0, 1, 2]\n",
    "        self.currentSituation = 0\n",
    "    \n",
    "    def closeShort(self):\n",
    "        self.sampleActionHistory.append(6)\n",
    "        self.closePosition() \n",
    "        self.possibleNextActions = [0, 1, 2]\n",
    "        self.currentSituation = 0\n",
    "    \n",
    "    def closePosition(self):\n",
    "        self.sampleDoneHistory.append(True)\n",
    "        rewarder = Rewarder(self.currentSample, 12)\n",
    "        self.sampleRewardHistory = rewarder.getRewards(self.sampleActionHistory)\n",
    "        self.roiHistory.append(rewarder.getRoi(self.sampleActionHistory))\n",
    "\n",
    "class Rewarder:\n",
    "    # calculates the reward for any action\n",
    "    \n",
    "    def __init__(self, dataset, sphereSize):\n",
    "        self.dataset = dataset\n",
    "        self.sphereSize = sphereSize # defines how many neighbour bars we observe to estimate the goodness of the action\n",
    "        self.rewardFunctions = {0: self.rewardObserveMarket, 1: self.rewardOpenLong, 2: self.rewardOpenShort, \\\n",
    "                                3: self.rewardHoldLong, 4: self.rewardHoldShort, 5: self.rewardCloseLong, 6: self.rewardCloseShort}\n",
    "        self.orderPosition = None\n",
    "        self.closePosition = None\n",
    "        self.lengthPositionOpen = None\n",
    "        self.orderType = None\n",
    "        \n",
    "    def getRewards(self, actionHistory):\n",
    "        \n",
    "        rewardHistory = []\n",
    "        self.getOrderAndClosePosition(actionHistory)\n",
    "        position = self.dataset.numObtainBars - 1 # the start position is the number of obtained bars in the sample\n",
    "        \n",
    "        for action in actionHistory:\n",
    "            rewardFunctionHandle = self.rewardFunctions[action]\n",
    "            rewardHistory.append( rewardFunctionHandle(position) )\n",
    "            position += 1\n",
    "        \n",
    "        return rewardHistory\n",
    "        \n",
    "    def rewardObserveMarket(self, position):\n",
    "        # calculates the reward for waiting before opening a position\n",
    "        \n",
    "        return 0\n",
    "        \n",
    "    def rewardOpenLong(self, position):\n",
    "        # calculates the reward for opening a long position\n",
    "        # orderPosition and closePosition must be the value dataset returns via the getPosition method\n",
    "        \n",
    "        bestOpeningPrice = self.dataset.getPeakPrice(\"min\", self.orderPosition - self.sphereSize, self.closePosition)\n",
    "        realOpeningPrice = self.dataset.priceAsk[self.orderPosition - 1]\n",
    "        realClosingPrice = self.dataset.priceBid[self.closePosition - 1]\n",
    "        \n",
    "        roi = 100 * (realClosingPrice - realOpeningPrice) / realOpeningPrice\n",
    "        missedRoi = 100 * (realOpeningPrice - bestOpeningPrice) / realOpeningPrice\n",
    "        \n",
    "        reward = (0.5*roi - missedRoi) / self.lengthPositionOpen\n",
    "        return self.clipRewards(reward)\n",
    "    \n",
    "    def rewardOpenShort(self, position):\n",
    "        # calculates the reward for opening a short position\n",
    "        # orderPosition and closePosition must be the value dataset returns via the getPosition method  \n",
    "        \n",
    "        bestOpeningPrice = self.dataset.getPeakPrice(\"max\", self.orderPosition - self.sphereSize, self.closePosition)\n",
    "        realOpeningPrice = self.dataset.priceBid[self.orderPosition - 1]\n",
    "        realClosingPrice = self.dataset.priceAsk[self.closePosition - 1]\n",
    "        \n",
    "        roi = -100 * (realClosingPrice - realOpeningPrice) / realOpeningPrice\n",
    "        missedRoi = -100 * (realOpeningPrice - bestOpeningPrice) / realOpeningPrice\n",
    "        \n",
    "        reward = (0.5*roi - missedRoi) * (24/self.lengthPositionOpen)\n",
    "        return self.clipRewards(reward)  \n",
    "    \n",
    "    def rewardHoldLong(self, position):\n",
    "        # calculates the reward for waiting inside an open long position\n",
    "        # position must be the value the dataset object returns via the getPosition method\n",
    "        \n",
    "        currentSellPrice = self.dataset.priceBid[position - 1]\n",
    "        bestFutureSellPrice = self.dataset.getPeakPrice(\"max\", position - 1, self.closePosition)\n",
    "        worstFutureSellPrice = self.dataset.getPeakPrice(\"min\", position - 1, self.closePosition)\n",
    "        meanFutureSellPrice = (bestFutureSellPrice + worstFutureSellPrice) / 2\n",
    "        \n",
    "        reward = 100 * 2 * (meanFutureSellPrice - currentSellPrice) / currentSellPrice  * (24/self.lengthPositionOpen)\n",
    "        return self.clipRewards(reward)\n",
    "    \n",
    "    def rewardHoldShort(self, position):\n",
    "        # calculates the reward for waiting inside an open long position\n",
    "        # position must be the value the dataset object returns via the getPosition method\n",
    "        \n",
    "        currentBuyPrice = self.dataset.priceAsk[position - 1]\n",
    "        bestFutureBuyPrice = self.dataset.getPeakPrice(\"min\", position, self.closePosition)\n",
    "        worstFutureBuyPrice = self.dataset.getPeakPrice(\"max\", position, self.closePosition)\n",
    "        meanFutureSellPrice = (bestFutureBuyPrice + worstFutureBuyPrice) / 2\n",
    "        \n",
    "        reward = -100 * 2 * (meanFutureSellPrice - currentBuyPrice) / currentBuyPrice  * (24/self.lengthPositionOpen)\n",
    "        return self.clipRewards(reward) \n",
    "\n",
    "    def rewardCloseLong(self, position):\n",
    "        # calculates the reward for closing a long position\n",
    "        # orderPosition and closePosition must be the value dataset returns via the getPosition method\n",
    "        \n",
    "        bestClosingPrice = self.dataset.getPeakPrice(\"max\", self.orderPosition, self.closePosition + self.sphereSize)\n",
    "        realClosingPrice = self.dataset.priceBid[self.closePosition -1]\n",
    "        realOpeningPrice = self.dataset.priceAsk[self.orderPosition - 1]\n",
    "        \n",
    "        roi = 100 * (realClosingPrice - realOpeningPrice) / realOpeningPrice\n",
    "        missedRoi = 100 * (bestClosingPrice - realClosingPrice) / realOpeningPrice\n",
    "        \n",
    "        reward = (0.5*roi - missedRoi)  * (24/self.lengthPositionOpen)\n",
    "        return self.clipRewards(reward)\n",
    "    \n",
    "    def rewardCloseShort(self, position):\n",
    "        # calculates the reward for closing a short position\n",
    "        # orderPosition and closePosition must be the value dataset returns via the getPosition method    \n",
    "        \n",
    "        bestClosingPrice = self.dataset.getPeakPrice(\"min\", self.orderPosition, self.closePosition + self.sphereSize)\n",
    "        realClosingPrice = self.dataset.priceAsk[self.closePosition -1]\n",
    "        realOpeningPrice = self.dataset.priceBid[self.orderPosition - 1]\n",
    "        \n",
    "        roi = -100 * (realClosingPrice - realOpeningPrice) / realOpeningPrice\n",
    "        missedRoi = -100 * (bestClosingPrice - realClosingPrice) / realClosingPrice\n",
    "        \n",
    "        reward = (0.5*roi - missedRoi)  * (24/self.lengthPositionOpen)\n",
    "        return self.clipRewards(reward)        \n",
    "        \n",
    "    def getRoi(self, actionHistory):\n",
    "        # calculates the relative return on investment from the action history of the sample\n",
    "        \n",
    "        self.getOrderAndClosePosition(actionHistory)\n",
    "        \n",
    "        if self.orderType == 1:\n",
    "            openingPrice = self.dataset.priceAsk[self.orderPosition - 1]\n",
    "            closingPrice = self.dataset.priceBid[self.closePosition - 1]\n",
    "            roi = (closingPrice - openingPrice) / openingPrice\n",
    "        else:\n",
    "            openingPrice = self.dataset.priceBid[self.orderPosition - 1]\n",
    "            closingPrice = self.dataset.priceAsk[self.closePosition - 1]\n",
    "            roi = -(closingPrice - openingPrice) / openingPrice\n",
    "        \n",
    "        return roi\n",
    "    \n",
    "    def getOrderAndClosePosition(self, actionHistory):\n",
    "        # returns the positions of the order and the close position inside the sample and if the order was a long (1) or short (-1)\n",
    "        \n",
    "        actionHistory = np.array(actionHistory)\n",
    "        \n",
    "        orderLongPos = np.where(actionHistory == 1)[0].tolist()\n",
    "        orderShortPos = np.where(actionHistory == 2)[0].tolist()\n",
    "        closeLongPos = np.where(actionHistory == 5)[0].tolist()\n",
    "        closeShortPos = np.where(actionHistory == 6)[0].tolist()\n",
    "        \n",
    "        orderPos = orderLongPos + orderShortPos\n",
    "        closingPos = closeLongPos + closeShortPos\n",
    "        \n",
    "        orderType = 1 if len(orderLongPos) != 0 else -1\n",
    "        \n",
    "        sampleOffset = self.dataset.numObtainBars - 1\n",
    "        self.orderPosition = orderPos[0] + sampleOffset\n",
    "        self.closePosition = closingPos[0] + sampleOffset\n",
    "        self.lengthPositionOpen = self.closePosition - self.orderPosition\n",
    "        self.orderType = orderType\n",
    "    \n",
    "    @staticmethod\n",
    "    def clipRewards(reward):\n",
    "        # uses a sigmoid function to clip rewards from -1 to +1\n",
    "        \n",
    "        clippedReward = 2 / (1 + np.exp(-14*reward)) - 1\n",
    "        return clippedReward\n",
    "\n",
    "class Sampler:\n",
    "    # returns a random dataset given the boundaries defined in the __init__ method\n",
    "    \n",
    "    def __init__(self, resolution):\n",
    "        if resolution < 2: raise ValueError(\"Only minute (2), hour (3) and daily (4) data\")\n",
    "        \n",
    "        self.qb = QuantBook()\n",
    "        self.forexList = [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCAD\", \"AUSUSD\", \"USDCHF\", \\\n",
    "                          \"NZDUSD\", \"EURGBP\", \"EURJPY\", \"AUDJPY\", \"GBPJPY\", \"EURCHF\"]\n",
    "        self.MINIMUM_YEAR = 2008\n",
    "        self.MAXIMUM_YEAR = 2015\n",
    "        self.resolution = resolution\n",
    "        self.MAX_RANDOMIZED_SAMPLE_LENGTH = {2:1440, 3:24, 4:0} # depending on resolution\n",
    "        self.NUM_SAMPLES = 720\n",
    "        self.WARUMUP_BUFFER_SAMPLES = 50\n",
    "        self.DAYS_MONTH = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, \\\n",
    "                           7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "        self.rsi = None\n",
    "        self.macd = None\n",
    "        self.standardDeviation = None\n",
    "        self.adx = None\n",
    "        self.addTradingObjects()\n",
    "        self.addIndicators()\n",
    "        \n",
    "    def addTradingObjects(self):\n",
    "        for fxSymbol in self.forexList:\n",
    "            self.qb.AddForex(fxSymbol)\n",
    "        \n",
    "    def addIndicators(self):\n",
    "        self.rsi = RelativeStrengthIndex(14)\n",
    "        self.macd = MovingAverageConvergenceDivergence(12, 26, 9)\n",
    "        self.standardDeviation = StandardDeviation(26)\n",
    "        self.adx = AverageDirectionalIndex(\"ADX\", 18)\n",
    "    \n",
    "    def getSample(self, snapshotLength):\n",
    "        \n",
    "        dataNotAvailable = True\n",
    "        \n",
    "        while dataNotAvailable:\n",
    "            forexSymbol = self.getRandomForexSymbol()\n",
    "            datetimeEnd = self.getRandomDatetime()\n",
    "            self.qb.SetStartDate(datetimeEnd)\n",
    "            randomSampleLength = random.randint(0, self.MAX_RANDOMIZED_SAMPLE_LENGTH[self.resolution]) # randomize the sample length to not start at the same time for each sample\n",
    "            sampleLength = self.NUM_SAMPLES + randomSampleLength\n",
    "            dataframeLength = sampleLength + self.WARUMUP_BUFFER_SAMPLES\n",
    "            \n",
    "            price = self.qb.History([forexSymbol], dataframeLength, self.resolution)\n",
    "            dataNotAvailable = price.empty\n",
    "            \n",
    "        sample = Dataset(sampleLength, snapshotLength)\n",
    "        sample.setPrice( self.qb.History([forexSymbol], dataframeLength, self.resolution) )\n",
    "        sample.setRsi( self.qb.Indicator(self.rsi, forexSymbol, dataframeLength, self.resolution) )\n",
    "        sample.setAdx( self.qb.Indicator(self.adx, forexSymbol, dataframeLength, self.resolution) )\n",
    "        sample.setMacd( self.qb.Indicator(self.macd, forexSymbol, dataframeLength, self.resolution), \\\n",
    "                       self.qb.Indicator(self.standardDeviation, forexSymbol, dataframeLength, self.resolution) )\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def getRandomForexSymbol(self):\n",
    "        nSymbols = len(self.forexList)\n",
    "        randomForexNumber = random.randint(0, nSymbols-1)\n",
    "        return self.forexList[randomForexNumber]\n",
    "    \n",
    "    def getRandomDatetime(self, noMaximumLimitation = False):\n",
    "        datetimeNow = datetime.now()\n",
    "        maximumYear = datetimeNow.year if noMaximumLimitation else self.MAXIMUM_YEAR            \n",
    "        randomYear = random.randint(self.MINIMUM_YEAR, maximumYear)\n",
    "        randomMonth = random.randint(1, 12) if randomYear != datetimeNow.year else random.randint(1, datetimeNow.month)\n",
    "        nDaysMonth = self.DAYS_MONTH[randomMonth] if not (self.isLeapYear(randomYear) and randomMonth == 2) else 29\n",
    "        randomDay = random.randint(1, nDaysMonth)\n",
    "        \n",
    "        randomDatetime = datetime(randomYear, randomMonth, randomDay)\n",
    "        latestDatetime = datetimeNow\n",
    "        return min(randomDatetime, latestDatetime)\n",
    "    \n",
    "    def getSamplesRangeTimedelta(self):\n",
    "        resolutionMinutes = self.RESOLUTION_MINUTES[self.resolution]\n",
    "        minimumMinutes = resolutionMinutes * (self.NUM_SAMPLES + self.WARUMUP_BUFFER_SAMPLES)\n",
    "        return timedelta(minutes = minimumMinutes)\n",
    "    \n",
    "    @staticmethod\n",
    "    def isLeapYear(year):\n",
    "        return True if year % 4 == 0 else False\n",
    "\n",
    "class Dataset:\n",
    "    ''' \n",
    "    The Dataset class is a wrapper for a data sample. Once the required dataframes are set,\n",
    "    we can loop through the data, revealing us a snapshot of the last observed datapoints.\n",
    "    The amount of datapoints revealed in one snapshot can be adjusted with the variable numObtainBars.\n",
    "    Every step we iterate over a dataset the window steps one datapoint forward (as in a queue).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, numBars, numObtainBars):\n",
    "        self.length = numBars # defines how many bars with the beginning of the last datapoint we'll ingest from the dataframes\n",
    "        self.numObtainBars = numObtainBars # defines how many last bars we should return when the iterator is called\n",
    "        self.priceAsk = []\n",
    "        self.priceBid = []\n",
    "        self.rsi = []\n",
    "        self.macdLine = []\n",
    "        self.macdHistogram = []\n",
    "        self.adx = []\n",
    "        self.iterPosition = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.iterPosition = self.numObtainBars - 1\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.iterPosition += 1\n",
    "        if self.iterPosition > self.length: raise StopIteration \n",
    "\n",
    "        startPoint = self.iterPosition - self.numObtainBars\n",
    "        endPoint = self.iterPosition\n",
    "        snapshot = self.createSnapshot(startPoint, endPoint)\n",
    "        return snapshot\n",
    "    \n",
    "    def getPosition(self):\n",
    "        return self.iterPosition\n",
    "    \n",
    "    def getPeakPrice(self, maxMin, startPoint, endPoint):\n",
    "        # returns the maximum bid price / minimum ask price with respect to the referencePoint to the next <deltaBars> samples\n",
    "                \n",
    "        if maxMin == \"min\":\n",
    "            return min( self.priceAsk[startPoint:endPoint] )\n",
    "        elif maxMin == \"max\":\n",
    "            return max( self.priceBid[startPoint:endPoint] )\n",
    "        else:\n",
    "            raise ValueError(\"maxMin must be defined by codewords max or min\")\n",
    "    \n",
    "    def createSnapshot(self, startPoint, endPoint):\n",
    "        snapshot = [ self.rsi[startPoint:endPoint], self.macdLine[startPoint:endPoint], \\\n",
    "                    self.macdHistogram[startPoint:endPoint], self.adx[startPoint:endPoint] ]         \n",
    "        return snapshot\n",
    "    \n",
    "    def setPrice(self, priceDataframe):\n",
    "        self.priceAsk = priceDataframe['askclose'].values[-self.length:]\n",
    "        self.priceBid = priceDataframe['bidclose'].values[-self.length:]\n",
    "    \n",
    "    def setRsi(self, rsiDataframe):\n",
    "        self.rsi = rsiDataframe['relativestrengthindex'].values[-self.length:]\n",
    "    \n",
    "    def setAdx(self, adxDataframe):\n",
    "        self.adx = adxDataframe['averagedirectionalindex'].values[-self.length:]\n",
    "    \n",
    "    def setMacd(self, macdDataframe, stddevDataframe):\n",
    "        macdLine = macdDataframe['movingaverageconvergencedivergence'].values[-self.length:]\n",
    "        macdHistogram = macdDataframe['histogram'].values[-self.length:]\n",
    "        standardDeviation = stddevDataframe['standarddeviation'].values[-self.length:]\n",
    "        self.macdLine = [macd/stddev for macd, stddev in zip(macdLine, standardDeviation)] # norm the data due to comparison between different price levels\n",
    "        self.macdHistogram = [histogram/stddev for histogram, stddev in zip(macdHistogram, standardDeviation)] # norm the data due to comparison between different price levels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 finished. ROI: 0.13\n",
      "episode 2 finished. ROI: -0.19\n",
      "episode 3 finished. ROI: -0.77\n",
      "episode 4 finished. ROI: 0.38\n",
      "episode 5 finished. ROI: -0.29\n",
      "episode 6 finished. ROI: -0.09\n",
      "episode 7 finished. ROI: -0.06\n",
      "episode 8 finished. ROI: -0.84\n",
      "episode 9 finished. ROI: 0.59\n",
      "episode 10 finished. ROI: -0.14\n",
      "episode 11 finished. ROI: -0.21\n",
      "episode 12 finished. ROI: 0.06\n",
      "episode 13 finished. ROI: 0.24\n",
      "episode 14 finished. ROI: 0.11\n",
      "episode 15 finished. ROI: 0.38\n",
      "episode 16 finished. ROI: -1.22\n",
      "episode 17 finished. ROI: 0.27\n",
      "episode 18 finished. ROI: -0.1\n",
      "episode 19 finished. ROI: 0.44\n",
      "episode 20 finished. ROI: -0.46\n",
      "episode 21 finished. ROI: 1.18\n",
      "episode 22 finished. ROI: -0.5\n",
      "episode 23 finished. ROI: 1.0\n",
      "episode 24 finished. ROI: 0.89\n",
      "episode 25 finished. ROI: 0.24\n",
      "episode 26 finished. ROI: -0.11\n",
      "episode 27 finished. ROI: 0.47\n",
      "episode 28 finished. ROI: -0.23\n",
      "episode 29 finished. ROI: -0.39\n",
      "episode 30 finished. ROI: -0.18\n",
      "episode 31 finished. ROI: -1.0\n",
      "episode 32 finished. ROI: -0.54\n",
      "episode 33 finished. ROI: -0.44\n",
      "episode 34 finished. ROI: -0.03\n",
      "episode 35 finished. ROI: 0.38\n",
      "episode 36 finished. ROI: -0.35\n",
      "episode 37 finished. ROI: -0.11\n",
      "episode 38 finished. ROI: 0.07\n",
      "episode 39 finished. ROI: -0.47\n",
      "episode 40 finished. ROI: -0.3\n",
      "episode 41 finished. ROI: 0.12\n",
      "episode 42 finished. ROI: 0.6\n",
      "episode 43 finished. ROI: 0.23\n",
      "episode 44 finished. ROI: 0.36\n",
      "episode 45 finished. ROI: 0.26\n",
      "episode 46 finished. ROI: -0.07\n",
      "episode 47 finished. ROI: -0.05\n",
      "episode 48 finished. ROI: 0.32\n",
      "episode 49 finished. ROI: 0.3\n",
      "episode 50 finished. ROI: -0.03\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(24)\n",
    "session = tf.Session()\n",
    "agent.run(50, session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
